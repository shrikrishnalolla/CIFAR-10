{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "35GOw1-Sphtv"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-YTEhJG0plaz"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y1BWVS2kpp5E"
   },
   "outputs": [],
   "source": [
    "#importing dataset\n",
    "(X_train,Y_train),(X_test,Y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pPzzsTl7CGl3"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train/=255 #Standardise the data\n",
    "X_test/=255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HZ8v9jleCKor"
   },
   "outputs": [],
   "source": [
    "Y_train = tf.keras.utils.to_categorical(Y_train)\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "Ony_VFGKq6hs",
    "outputId": "c7cb3d90-a3fc-4e69-e109-3361cc24be09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RvOcvhunrITO"
   },
   "outputs": [],
   "source": [
    "#50000 train examples each of size 32 x 32 x 3\n",
    "#10000 test examples each of size 32 x 32 x 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AMzMOUD2ok5O"
   },
   "source": [
    "Image size : 32 x 32 x 3\n",
    "\n",
    "Training set size : 50000\n",
    "\n",
    "Test set size     : 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UtHdaIOQnf7l"
   },
   "source": [
    "NETWORK ARCHITECTURE:\n",
    "\n",
    "The network has 3 convolutional blocks.\n",
    "\n",
    "Block 1 : \n",
    "\n",
    "3 Convolutional Layers each having 32 filters of size 3 x 3. \n",
    "The padding is 'same'.Each filter is initialised using 'he_uniform' initializer.\n",
    "Relu activation has been used.Batch Normalization is applied at the end of each layer.\n",
    "\n",
    "Max Pooling : Size 2 x 2 with stride 2.\n",
    "\n",
    "Droupout Regularisation : Drop Probability = 0.5\n",
    "\n",
    "Block 2 : \n",
    "\n",
    "2 Convolutional Layers each having 64 filters of size 5 x 5. \n",
    "The padding is 'same'.Each filter is initialised using 'he_uniform' initializer.\n",
    "Relu activation has been used.Batch Normalization is applied at the end of each layer.\n",
    "\n",
    "Max Pooling : Size 2 x 2 with stride 2.\n",
    "\n",
    "Droupout Regularisation : Drop Probability = 0.5\n",
    "\n",
    "Block 3 :\n",
    "\n",
    "2 Convolutional Layers each having 256 filters of size 5 x 5. \n",
    "The padding is 'same'.Each filter is initialised using 'he_uniform' initializer.\n",
    "Relu activation has been used.Batch Normalization is applied at the end of the block.\n",
    "\n",
    "Max Pooling : size 2 x 2 with stride 2\n",
    "\n",
    "Droupout Regularisation : Drop Probability = 0.5\n",
    "\n",
    "After this , the layers are flattened and fed into Fully Connected Layers\n",
    "\n",
    "There are 3 Fully Connected Layers.Each of them is followed by Batch Normalization(except the last layer which is the output layer).\n",
    "\n",
    "Layer 1 : 128 neurons, relu activation, he_uniform initializer\n",
    "\n",
    "Layer 2 : 32 neurons, relu activation, he_uniform initializer\n",
    "\n",
    "Layer 3 : 10 neurons(representing 10 classes of the dataset).It has a softmax output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QeUGIrx6s7DD"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "#Block 1 : (CONV->Relu->Batch_Norm) X 3\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=32,kernel_size=(3,3),padding='same',kernel_initializer='he_uniform',input_shape=X_train.shape[1:]))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=32,kernel_size=(3,3),padding='same',kernel_initializer='he_uniform'))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=32,kernel_size=(3,3),padding='same',kernel_initializer='he_uniform'))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "#Pooling and Droupout\n",
    "\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2),strides=2))\n",
    "model.add(tf.keras.layers.Dropout(0.50))\n",
    "\n",
    "#Block 2 : (CONV->Relu->Batch_Norm) X 2 \n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=64,kernel_size=(5,5),padding='same',kernel_initializer='he_uniform'))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=64,kernel_size=(5,5),padding='same',kernel_initializer='he_uniform'))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "#Pooling and Droupout\n",
    "\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2),strides=2))\n",
    "model.add(tf.keras.layers.Dropout(0.50))\n",
    "\n",
    "#Block 3 : (CONV->Relu->Batch_Norm) X 2\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=256,kernel_size=(5,5),padding='same',kernel_initializer='he_uniform'))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=256,kernel_size=(5,5),padding='same',kernel_initializer='he_uniform'))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "#Pooling and Droupout\n",
    "\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2),strides=2))\n",
    "model.add(tf.keras.layers.Dropout(0.50))\n",
    "\n",
    "#Fully Connected Layers : (Dense_layer(Relu)->Batch_Norm)\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128,activation='relu',kernel_initializer='he_uniform'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(32,activation='relu',kernel_initializer='he_uniform'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(10,activation=\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "KokiU0dwRHVi",
    "outputId": "47d8cfae-6a30-4512-ce7a-b059640fceec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        51264     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 256)         409856    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 256)         1638656   \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               524416    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 2,754,090\n",
      "Trainable params: 2,752,298\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e6afPIXnuINY"
   },
   "source": [
    "IMAGE AUGMENTATION:\n",
    "\n",
    "The Trainset is fed into ImageDataGenerator for augmentation.\n",
    "\n",
    "Rotation range : 15\n",
    "\n",
    "Width and Height shift range : 0.2\n",
    "\n",
    "Zoom range : 0.2\n",
    "\n",
    "Horizontal flipping is allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xVcruT8PTsww"
   },
   "outputs": [],
   "source": [
    "image = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=15, width_shift_range=0.2,height_shift_range=0.2,zoom_range=0.2,horizontal_flip=True)\n",
    "image.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DpqRVKdKupus",
    "outputId": "2474e905-7099-4dda-ee60-38a66d689623"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function : Categorical crossentropy\n",
    "\n",
    "Optimizer     : Adam , learning rate = 0.013, all other parameters remain the same.\n",
    "\n",
    "Batch size    : 64\n",
    "\n",
    "epochs        : 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "v9lWdshX-__q",
    "outputId": "6f3308bd-2a8d-4a47-952e-3fbb177c4cda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-b27bbe9649c1>:4: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/200\n",
      "782/781 [==============================] - 29s 37ms/step - loss: 1.7030 - accuracy: 0.3809 - val_loss: 1.7430 - val_accuracy: 0.4268\n",
      "Epoch 2/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 1.3488 - accuracy: 0.5138 - val_loss: 1.2036 - val_accuracy: 0.5793\n",
      "Epoch 3/200\n",
      "782/781 [==============================] - 29s 37ms/step - loss: 1.1581 - accuracy: 0.5890 - val_loss: 1.0722 - val_accuracy: 0.6347\n",
      "Epoch 4/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 1.0309 - accuracy: 0.6368 - val_loss: 0.8860 - val_accuracy: 0.6883\n",
      "Epoch 5/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.9536 - accuracy: 0.6670 - val_loss: 0.8589 - val_accuracy: 0.7069\n",
      "Epoch 6/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.8867 - accuracy: 0.6894 - val_loss: 0.7908 - val_accuracy: 0.7306\n",
      "Epoch 7/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.8412 - accuracy: 0.7074 - val_loss: 0.9142 - val_accuracy: 0.6911\n",
      "Epoch 8/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.8032 - accuracy: 0.7219 - val_loss: 0.6897 - val_accuracy: 0.7653\n",
      "Epoch 9/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.7690 - accuracy: 0.7346 - val_loss: 0.6684 - val_accuracy: 0.7663\n",
      "Epoch 10/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.7444 - accuracy: 0.7442 - val_loss: 0.7430 - val_accuracy: 0.7472\n",
      "Epoch 11/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.7292 - accuracy: 0.7462 - val_loss: 0.6130 - val_accuracy: 0.7894\n",
      "Epoch 12/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.6992 - accuracy: 0.7569 - val_loss: 0.6872 - val_accuracy: 0.7667\n",
      "Epoch 13/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.6855 - accuracy: 0.7602 - val_loss: 0.7268 - val_accuracy: 0.7574\n",
      "Epoch 14/200\n",
      "782/781 [==============================] - 29s 37ms/step - loss: 0.6663 - accuracy: 0.7696 - val_loss: 0.6217 - val_accuracy: 0.7889\n",
      "Epoch 15/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.6491 - accuracy: 0.7759 - val_loss: 0.5337 - val_accuracy: 0.8196\n",
      "Epoch 16/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.6418 - accuracy: 0.7779 - val_loss: 0.5518 - val_accuracy: 0.8100\n",
      "Epoch 17/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.6244 - accuracy: 0.7838 - val_loss: 0.5447 - val_accuracy: 0.8167\n",
      "Epoch 18/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.6130 - accuracy: 0.7882 - val_loss: 0.6127 - val_accuracy: 0.7950\n",
      "Epoch 19/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.6068 - accuracy: 0.7896 - val_loss: 0.5960 - val_accuracy: 0.7980\n",
      "Epoch 20/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.5910 - accuracy: 0.7961 - val_loss: 0.5673 - val_accuracy: 0.8124\n",
      "Epoch 21/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.5885 - accuracy: 0.7963 - val_loss: 0.5971 - val_accuracy: 0.8003\n",
      "Epoch 22/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.5797 - accuracy: 0.8011 - val_loss: 0.4856 - val_accuracy: 0.8347\n",
      "Epoch 23/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.5709 - accuracy: 0.8028 - val_loss: 0.4923 - val_accuracy: 0.8340\n",
      "Epoch 24/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.5649 - accuracy: 0.8035 - val_loss: 0.5026 - val_accuracy: 0.8325\n",
      "Epoch 25/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.5586 - accuracy: 0.8067 - val_loss: 0.5503 - val_accuracy: 0.8164\n",
      "Epoch 26/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.5512 - accuracy: 0.8083 - val_loss: 0.4714 - val_accuracy: 0.8404\n",
      "Epoch 27/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.5446 - accuracy: 0.8110 - val_loss: 0.4620 - val_accuracy: 0.8461\n",
      "Epoch 28/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.5380 - accuracy: 0.8137 - val_loss: 0.4896 - val_accuracy: 0.8339\n",
      "Epoch 29/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.5284 - accuracy: 0.8183 - val_loss: 0.5098 - val_accuracy: 0.8283\n",
      "Epoch 30/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.5262 - accuracy: 0.8185 - val_loss: 0.4329 - val_accuracy: 0.8531\n",
      "Epoch 31/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.5194 - accuracy: 0.8214 - val_loss: 0.5748 - val_accuracy: 0.8087\n",
      "Epoch 32/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.5197 - accuracy: 0.8216 - val_loss: 0.4701 - val_accuracy: 0.8423\n",
      "Epoch 33/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.5085 - accuracy: 0.8227 - val_loss: 0.4943 - val_accuracy: 0.8380\n",
      "Epoch 34/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.5072 - accuracy: 0.8242 - val_loss: 0.4570 - val_accuracy: 0.8498\n",
      "Epoch 35/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.5031 - accuracy: 0.8260 - val_loss: 0.5254 - val_accuracy: 0.8295\n",
      "Epoch 36/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.5006 - accuracy: 0.8276 - val_loss: 0.4483 - val_accuracy: 0.8486\n",
      "Epoch 37/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4918 - accuracy: 0.8288 - val_loss: 0.4113 - val_accuracy: 0.8611\n",
      "Epoch 38/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4880 - accuracy: 0.8328 - val_loss: 0.5599 - val_accuracy: 0.8218\n",
      "Epoch 39/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4841 - accuracy: 0.8327 - val_loss: 0.4426 - val_accuracy: 0.8504\n",
      "Epoch 40/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4839 - accuracy: 0.8338 - val_loss: 0.5864 - val_accuracy: 0.8084\n",
      "Epoch 41/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4818 - accuracy: 0.8344 - val_loss: 0.4754 - val_accuracy: 0.8413\n",
      "Epoch 42/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4745 - accuracy: 0.8342 - val_loss: 0.4289 - val_accuracy: 0.8570\n",
      "Epoch 43/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4776 - accuracy: 0.8364 - val_loss: 0.4949 - val_accuracy: 0.8390\n",
      "Epoch 44/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4640 - accuracy: 0.8405 - val_loss: 0.4251 - val_accuracy: 0.8577\n",
      "Epoch 45/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4600 - accuracy: 0.8410 - val_loss: 0.4327 - val_accuracy: 0.8559\n",
      "Epoch 46/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4627 - accuracy: 0.8399 - val_loss: 0.4243 - val_accuracy: 0.8611\n",
      "Epoch 47/200\n",
      "782/781 [==============================] - 29s 37ms/step - loss: 0.4587 - accuracy: 0.8410 - val_loss: 0.4792 - val_accuracy: 0.8430\n",
      "Epoch 48/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4550 - accuracy: 0.8415 - val_loss: 0.4507 - val_accuracy: 0.8487\n",
      "Epoch 49/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4518 - accuracy: 0.8427 - val_loss: 0.4127 - val_accuracy: 0.8629\n",
      "Epoch 50/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4442 - accuracy: 0.8461 - val_loss: 0.4122 - val_accuracy: 0.8593\n",
      "Epoch 51/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4504 - accuracy: 0.8447 - val_loss: 0.4895 - val_accuracy: 0.8425\n",
      "Epoch 52/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4416 - accuracy: 0.8488 - val_loss: 0.4003 - val_accuracy: 0.8649\n",
      "Epoch 53/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4391 - accuracy: 0.8466 - val_loss: 0.4146 - val_accuracy: 0.8630\n",
      "Epoch 54/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4400 - accuracy: 0.8471 - val_loss: 0.4359 - val_accuracy: 0.8545\n",
      "Epoch 55/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4416 - accuracy: 0.8475 - val_loss: 0.4076 - val_accuracy: 0.8667\n",
      "Epoch 56/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4338 - accuracy: 0.8495 - val_loss: 0.4554 - val_accuracy: 0.8523\n",
      "Epoch 57/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4328 - accuracy: 0.8479 - val_loss: 0.4455 - val_accuracy: 0.8534\n",
      "Epoch 58/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4235 - accuracy: 0.8538 - val_loss: 0.5031 - val_accuracy: 0.8376\n",
      "Epoch 59/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4266 - accuracy: 0.8517 - val_loss: 0.4175 - val_accuracy: 0.8626\n",
      "Epoch 60/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4247 - accuracy: 0.8527 - val_loss: 0.3818 - val_accuracy: 0.8750\n",
      "Epoch 61/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4231 - accuracy: 0.8524 - val_loss: 0.4435 - val_accuracy: 0.8537\n",
      "Epoch 62/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4211 - accuracy: 0.8545 - val_loss: 0.4200 - val_accuracy: 0.8610\n",
      "Epoch 63/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4160 - accuracy: 0.8552 - val_loss: 0.4347 - val_accuracy: 0.8588\n",
      "Epoch 64/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4165 - accuracy: 0.8556 - val_loss: 0.4917 - val_accuracy: 0.8441\n",
      "Epoch 65/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4111 - accuracy: 0.8580 - val_loss: 0.4238 - val_accuracy: 0.8571\n",
      "Epoch 66/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4112 - accuracy: 0.8558 - val_loss: 0.3879 - val_accuracy: 0.8689\n",
      "Epoch 67/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4101 - accuracy: 0.8577 - val_loss: 0.4112 - val_accuracy: 0.8634\n",
      "Epoch 68/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4014 - accuracy: 0.8603 - val_loss: 0.3983 - val_accuracy: 0.8690\n",
      "Epoch 69/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4070 - accuracy: 0.8582 - val_loss: 0.3831 - val_accuracy: 0.8718\n",
      "Epoch 70/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4107 - accuracy: 0.8580 - val_loss: 0.4582 - val_accuracy: 0.8543\n",
      "Epoch 71/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.4038 - accuracy: 0.8594 - val_loss: 0.3797 - val_accuracy: 0.8729\n",
      "Epoch 72/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3955 - accuracy: 0.8621 - val_loss: 0.3797 - val_accuracy: 0.8753\n",
      "Epoch 73/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3979 - accuracy: 0.8619 - val_loss: 0.3641 - val_accuracy: 0.8781\n",
      "Epoch 74/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3959 - accuracy: 0.8624 - val_loss: 0.3774 - val_accuracy: 0.8758\n",
      "Epoch 75/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3926 - accuracy: 0.8636 - val_loss: 0.3884 - val_accuracy: 0.8711\n",
      "Epoch 76/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3946 - accuracy: 0.8620 - val_loss: 0.3746 - val_accuracy: 0.8775\n",
      "Epoch 77/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3892 - accuracy: 0.8631 - val_loss: 0.4005 - val_accuracy: 0.8708\n",
      "Epoch 78/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3863 - accuracy: 0.8659 - val_loss: 0.3599 - val_accuracy: 0.8822\n",
      "Epoch 79/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3850 - accuracy: 0.8650 - val_loss: 0.3744 - val_accuracy: 0.8775\n",
      "Epoch 80/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3886 - accuracy: 0.8652 - val_loss: 0.3962 - val_accuracy: 0.8727\n",
      "Epoch 81/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3847 - accuracy: 0.8645 - val_loss: 0.4307 - val_accuracy: 0.8648\n",
      "Epoch 82/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3801 - accuracy: 0.8672 - val_loss: 0.3648 - val_accuracy: 0.8812\n",
      "Epoch 83/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3810 - accuracy: 0.8674 - val_loss: 0.4334 - val_accuracy: 0.8659\n",
      "Epoch 84/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3798 - accuracy: 0.8685 - val_loss: 0.3708 - val_accuracy: 0.8798\n",
      "Epoch 85/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3770 - accuracy: 0.8682 - val_loss: 0.4361 - val_accuracy: 0.8605\n",
      "Epoch 86/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3741 - accuracy: 0.8681 - val_loss: 0.3885 - val_accuracy: 0.8710\n",
      "Epoch 87/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3742 - accuracy: 0.8700 - val_loss: 0.3870 - val_accuracy: 0.8708\n",
      "Epoch 88/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3732 - accuracy: 0.8693 - val_loss: 0.3720 - val_accuracy: 0.8777\n",
      "Epoch 89/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3732 - accuracy: 0.8694 - val_loss: 0.3702 - val_accuracy: 0.8808\n",
      "Epoch 90/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3731 - accuracy: 0.8700 - val_loss: 0.3863 - val_accuracy: 0.8767\n",
      "Epoch 91/200\n",
      "782/781 [==============================] - 29s 37ms/step - loss: 0.3678 - accuracy: 0.8727 - val_loss: 0.3603 - val_accuracy: 0.8823\n",
      "Epoch 92/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3718 - accuracy: 0.8702 - val_loss: 0.3648 - val_accuracy: 0.8818\n",
      "Epoch 93/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3670 - accuracy: 0.8731 - val_loss: 0.3845 - val_accuracy: 0.8765\n",
      "Epoch 94/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3715 - accuracy: 0.8715 - val_loss: 0.3953 - val_accuracy: 0.8730\n",
      "Epoch 95/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3605 - accuracy: 0.8737 - val_loss: 0.3456 - val_accuracy: 0.8862\n",
      "Epoch 96/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3655 - accuracy: 0.8730 - val_loss: 0.3592 - val_accuracy: 0.8807\n",
      "Epoch 97/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3681 - accuracy: 0.8715 - val_loss: 0.3758 - val_accuracy: 0.8788\n",
      "Epoch 98/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3589 - accuracy: 0.8746 - val_loss: 0.3826 - val_accuracy: 0.8785\n",
      "Epoch 99/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3580 - accuracy: 0.8744 - val_loss: 0.3903 - val_accuracy: 0.8729\n",
      "Epoch 100/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3542 - accuracy: 0.8761 - val_loss: 0.4836 - val_accuracy: 0.8488\n",
      "Epoch 101/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3566 - accuracy: 0.8748 - val_loss: 0.3909 - val_accuracy: 0.8743\n",
      "Epoch 102/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3514 - accuracy: 0.8767 - val_loss: 0.3763 - val_accuracy: 0.8808\n",
      "Epoch 103/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3574 - accuracy: 0.8748 - val_loss: 0.3505 - val_accuracy: 0.8857\n",
      "Epoch 104/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3564 - accuracy: 0.8763 - val_loss: 0.3538 - val_accuracy: 0.8841\n",
      "Epoch 105/200\n",
      "782/781 [==============================] - 28s 35ms/step - loss: 0.3569 - accuracy: 0.8741 - val_loss: 0.3938 - val_accuracy: 0.8683\n",
      "Epoch 106/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3510 - accuracy: 0.8779 - val_loss: 0.3663 - val_accuracy: 0.8834\n",
      "Epoch 107/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3554 - accuracy: 0.8768 - val_loss: 0.3488 - val_accuracy: 0.8857\n",
      "Epoch 108/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3537 - accuracy: 0.8757 - val_loss: 0.3771 - val_accuracy: 0.8780\n",
      "Epoch 109/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3506 - accuracy: 0.8778 - val_loss: 0.3548 - val_accuracy: 0.8852\n",
      "Epoch 110/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3517 - accuracy: 0.8773 - val_loss: 0.3453 - val_accuracy: 0.8883\n",
      "Epoch 111/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3456 - accuracy: 0.8788 - val_loss: 0.3369 - val_accuracy: 0.8888\n",
      "Epoch 112/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3435 - accuracy: 0.8805 - val_loss: 0.3585 - val_accuracy: 0.8837\n",
      "Epoch 113/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3429 - accuracy: 0.8796 - val_loss: 0.3642 - val_accuracy: 0.8818\n",
      "Epoch 114/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3464 - accuracy: 0.8796 - val_loss: 0.4202 - val_accuracy: 0.8685\n",
      "Epoch 115/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3479 - accuracy: 0.8785 - val_loss: 0.3344 - val_accuracy: 0.8906\n",
      "Epoch 116/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3410 - accuracy: 0.8810 - val_loss: 0.3390 - val_accuracy: 0.8923\n",
      "Epoch 117/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3437 - accuracy: 0.8806 - val_loss: 0.3773 - val_accuracy: 0.8801\n",
      "Epoch 118/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3416 - accuracy: 0.8800 - val_loss: 0.3707 - val_accuracy: 0.8812\n",
      "Epoch 119/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3394 - accuracy: 0.8817 - val_loss: 0.3373 - val_accuracy: 0.8895\n",
      "Epoch 120/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3373 - accuracy: 0.8812 - val_loss: 0.4570 - val_accuracy: 0.8641\n",
      "Epoch 121/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3372 - accuracy: 0.8823 - val_loss: 0.3871 - val_accuracy: 0.8755\n",
      "Epoch 122/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3368 - accuracy: 0.8836 - val_loss: 0.3929 - val_accuracy: 0.8728\n",
      "Epoch 123/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3344 - accuracy: 0.8838 - val_loss: 0.3882 - val_accuracy: 0.8786\n",
      "Epoch 124/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3355 - accuracy: 0.8826 - val_loss: 0.3645 - val_accuracy: 0.8808\n",
      "Epoch 125/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3320 - accuracy: 0.8844 - val_loss: 0.3218 - val_accuracy: 0.8948\n",
      "Epoch 126/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3343 - accuracy: 0.8831 - val_loss: 0.3626 - val_accuracy: 0.8856\n",
      "Epoch 127/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3380 - accuracy: 0.8804 - val_loss: 0.3740 - val_accuracy: 0.8783\n",
      "Epoch 128/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3336 - accuracy: 0.8834 - val_loss: 0.3800 - val_accuracy: 0.8788\n",
      "Epoch 129/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3325 - accuracy: 0.8827 - val_loss: 0.3282 - val_accuracy: 0.8910\n",
      "Epoch 130/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3302 - accuracy: 0.8846 - val_loss: 0.3694 - val_accuracy: 0.8800\n",
      "Epoch 131/200\n",
      "782/781 [==============================] - 28s 35ms/step - loss: 0.3292 - accuracy: 0.8849 - val_loss: 0.3773 - val_accuracy: 0.8827\n",
      "Epoch 132/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3280 - accuracy: 0.8859 - val_loss: 0.3743 - val_accuracy: 0.8831\n",
      "Epoch 133/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3243 - accuracy: 0.8871 - val_loss: 0.3528 - val_accuracy: 0.8866\n",
      "Epoch 134/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3272 - accuracy: 0.8846 - val_loss: 0.3563 - val_accuracy: 0.8870\n",
      "Epoch 135/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3278 - accuracy: 0.8867 - val_loss: 0.3465 - val_accuracy: 0.8866\n",
      "Epoch 136/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3263 - accuracy: 0.8858 - val_loss: 0.3489 - val_accuracy: 0.8897\n",
      "Epoch 137/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3247 - accuracy: 0.8859 - val_loss: 0.3621 - val_accuracy: 0.8860\n",
      "Epoch 138/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3240 - accuracy: 0.8860 - val_loss: 0.3583 - val_accuracy: 0.8863\n",
      "Epoch 139/200\n",
      "782/781 [==============================] - 28s 35ms/step - loss: 0.3223 - accuracy: 0.8881 - val_loss: 0.3835 - val_accuracy: 0.8777\n",
      "Epoch 140/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3263 - accuracy: 0.8858 - val_loss: 0.3225 - val_accuracy: 0.8961\n",
      "Epoch 141/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3204 - accuracy: 0.8871 - val_loss: 0.3507 - val_accuracy: 0.8902\n",
      "Epoch 142/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3198 - accuracy: 0.8883 - val_loss: 0.3865 - val_accuracy: 0.8776\n",
      "Epoch 143/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3193 - accuracy: 0.8873 - val_loss: 0.3818 - val_accuracy: 0.8791\n",
      "Epoch 144/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3181 - accuracy: 0.8902 - val_loss: 0.3587 - val_accuracy: 0.8868\n",
      "Epoch 145/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3200 - accuracy: 0.8882 - val_loss: 0.3205 - val_accuracy: 0.8945\n",
      "Epoch 146/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3161 - accuracy: 0.8894 - val_loss: 0.3522 - val_accuracy: 0.8865\n",
      "Epoch 147/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3152 - accuracy: 0.8886 - val_loss: 0.3230 - val_accuracy: 0.8969\n",
      "Epoch 148/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3149 - accuracy: 0.8885 - val_loss: 0.3416 - val_accuracy: 0.8894\n",
      "Epoch 149/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3153 - accuracy: 0.8888 - val_loss: 0.3802 - val_accuracy: 0.8810\n",
      "Epoch 150/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3099 - accuracy: 0.8906 - val_loss: 0.3485 - val_accuracy: 0.8892\n",
      "Epoch 151/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3086 - accuracy: 0.8918 - val_loss: 0.3840 - val_accuracy: 0.8787\n",
      "Epoch 152/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3180 - accuracy: 0.8890 - val_loss: 0.3839 - val_accuracy: 0.8778\n",
      "Epoch 153/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3111 - accuracy: 0.8918 - val_loss: 0.3542 - val_accuracy: 0.8850\n",
      "Epoch 154/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3085 - accuracy: 0.8904 - val_loss: 0.3336 - val_accuracy: 0.8908\n",
      "Epoch 155/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3123 - accuracy: 0.8907 - val_loss: 0.3468 - val_accuracy: 0.8914\n",
      "Epoch 156/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3137 - accuracy: 0.8911 - val_loss: 0.3794 - val_accuracy: 0.8771\n",
      "Epoch 157/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3099 - accuracy: 0.8925 - val_loss: 0.3422 - val_accuracy: 0.8908\n",
      "Epoch 158/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3087 - accuracy: 0.8911 - val_loss: 0.3588 - val_accuracy: 0.8841\n",
      "Epoch 159/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3080 - accuracy: 0.8916 - val_loss: 0.3454 - val_accuracy: 0.8888\n",
      "Epoch 160/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3094 - accuracy: 0.8917 - val_loss: 0.3212 - val_accuracy: 0.8970\n",
      "Epoch 161/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3031 - accuracy: 0.8942 - val_loss: 0.3402 - val_accuracy: 0.8941\n",
      "Epoch 162/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3056 - accuracy: 0.8929 - val_loss: 0.3383 - val_accuracy: 0.8923\n",
      "Epoch 163/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3038 - accuracy: 0.8934 - val_loss: 0.3327 - val_accuracy: 0.8954\n",
      "Epoch 164/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3006 - accuracy: 0.8943 - val_loss: 0.3532 - val_accuracy: 0.8898\n",
      "Epoch 165/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3042 - accuracy: 0.8943 - val_loss: 0.3203 - val_accuracy: 0.8989\n",
      "Epoch 166/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3036 - accuracy: 0.8942 - val_loss: 0.3617 - val_accuracy: 0.8869\n",
      "Epoch 167/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3063 - accuracy: 0.8928 - val_loss: 0.3663 - val_accuracy: 0.8841\n",
      "Epoch 168/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3014 - accuracy: 0.8955 - val_loss: 0.3480 - val_accuracy: 0.8895\n",
      "Epoch 169/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3016 - accuracy: 0.8947 - val_loss: 0.3420 - val_accuracy: 0.8917\n",
      "Epoch 170/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3044 - accuracy: 0.8934 - val_loss: 0.4044 - val_accuracy: 0.8748\n",
      "Epoch 171/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3009 - accuracy: 0.8940 - val_loss: 0.3530 - val_accuracy: 0.8902\n",
      "Epoch 172/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3038 - accuracy: 0.8943 - val_loss: 0.3502 - val_accuracy: 0.8890\n",
      "Epoch 173/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3000 - accuracy: 0.8950 - val_loss: 0.3436 - val_accuracy: 0.8940\n",
      "Epoch 174/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3030 - accuracy: 0.8936 - val_loss: 0.3372 - val_accuracy: 0.8966\n",
      "Epoch 175/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.2967 - accuracy: 0.8955 - val_loss: 0.3571 - val_accuracy: 0.8876\n",
      "Epoch 176/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.2977 - accuracy: 0.8953 - val_loss: 0.3202 - val_accuracy: 0.8952\n",
      "Epoch 177/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.3001 - accuracy: 0.8940 - val_loss: 0.3150 - val_accuracy: 0.8975\n",
      "Epoch 178/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.2967 - accuracy: 0.8959 - val_loss: 0.3616 - val_accuracy: 0.8864\n",
      "Epoch 179/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.2931 - accuracy: 0.8973 - val_loss: 0.3330 - val_accuracy: 0.8924\n",
      "Epoch 180/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.2943 - accuracy: 0.8970 - val_loss: 0.3667 - val_accuracy: 0.8814\n",
      "Epoch 181/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.2919 - accuracy: 0.8982 - val_loss: 0.3720 - val_accuracy: 0.8795\n",
      "Epoch 182/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.2935 - accuracy: 0.8972 - val_loss: 0.3714 - val_accuracy: 0.8868\n",
      "Epoch 183/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.2895 - accuracy: 0.8976 - val_loss: 0.3684 - val_accuracy: 0.8850\n",
      "Epoch 184/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.2928 - accuracy: 0.8968 - val_loss: 0.3537 - val_accuracy: 0.8894\n",
      "Epoch 185/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.2923 - accuracy: 0.8986 - val_loss: 0.3403 - val_accuracy: 0.8929\n",
      "Epoch 186/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.2930 - accuracy: 0.8979 - val_loss: 0.3283 - val_accuracy: 0.8925\n",
      "Epoch 187/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.2860 - accuracy: 0.8990 - val_loss: 0.3484 - val_accuracy: 0.8908\n",
      "Epoch 188/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.2964 - accuracy: 0.8943 - val_loss: 0.3583 - val_accuracy: 0.8858\n",
      "Epoch 189/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.2909 - accuracy: 0.8978 - val_loss: 0.3702 - val_accuracy: 0.8859\n",
      "Epoch 190/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.2865 - accuracy: 0.8990 - val_loss: 0.3300 - val_accuracy: 0.8958\n",
      "Epoch 191/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.2920 - accuracy: 0.8967 - val_loss: 0.3364 - val_accuracy: 0.8928\n",
      "Epoch 192/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.2872 - accuracy: 0.8988 - val_loss: 0.3708 - val_accuracy: 0.8876\n",
      "Epoch 193/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.2875 - accuracy: 0.8997 - val_loss: 0.3515 - val_accuracy: 0.8904\n",
      "Epoch 194/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.2860 - accuracy: 0.9005 - val_loss: 0.3419 - val_accuracy: 0.8939\n",
      "Epoch 195/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.2905 - accuracy: 0.8969 - val_loss: 0.3745 - val_accuracy: 0.8859\n",
      "Epoch 196/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.2862 - accuracy: 0.8997 - val_loss: 0.3453 - val_accuracy: 0.8943\n",
      "Epoch 197/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.2907 - accuracy: 0.8978 - val_loss: 0.3306 - val_accuracy: 0.8962\n",
      "Epoch 198/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.2904 - accuracy: 0.8984 - val_loss: 0.3818 - val_accuracy: 0.8830\n",
      "Epoch 199/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.2846 - accuracy: 0.9007 - val_loss: 0.3369 - val_accuracy: 0.8926\n",
      "Epoch 200/200\n",
      "782/781 [==============================] - 28s 36ms/step - loss: 0.2859 - accuracy: 0.8992 - val_loss: 0.3192 - val_accuracy: 0.8978\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5ea452ac50>"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=0.013)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "#model.fit(X_train,Y_train,validation_split=0.2,batch_size=1024,epochs=200)\n",
    "model.fit_generator(image.flow(X_train,Y_train,batch_size=64),steps_per_epoch=X_train.shape[0]/64,epochs=200,validation_data=(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "zsvXkjuV-lT9",
    "outputId": "1ebdede5-8250-4fea-aada-585f52ae9bf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 1s 10ms/step - loss: 0.3192 - accuracy: 0.8978\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.31920865178108215, 0.8978000283241272]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  model.evaluate(X_test, Y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain 89.92 % Train accuracy and 89.78% Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XMfAKUO8Nmdy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CIFAR-10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
